{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import matplotlib\n",
    "import calendar\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import plotly.io as pio\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import medfilt, butter, filtfilt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -c --timeout 10 https://physionet.org/static/published-projects/accelerometry-walk-climb-drive/labeled-raw-accelerometry-data-captured-during-walking-stair-climbing-and-driving-1.0.0.zip -O dataset.zip\n",
    "!unzip -d dataset -q dataset.zip\n",
    "!echo \"Done!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls dataset/labeled-raw-accelerometry-data-captured-during-walking-stair-climbing-and-driving-1.0.0/raw_accelerometry_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_single_file(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    return df   \n",
    "    # select by activity and plot them\n",
    "\n",
    "def read_all_data(folder_path):\n",
    "    # List to store the DataFrames\n",
    "    data_frames = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        \n",
    "        if filename.endswith(\".csv\"):\n",
    "            file_path = os.path.join(folder_path,filename)\n",
    "            df = pd.read_csv(file_path)\n",
    "            data_frames.append(df)\n",
    "        else:\n",
    "            print(f\"{filename} is not a csv file\")\n",
    "        \n",
    "    return data_frames \n",
    "    # print(df.head(5))\n",
    "\n",
    "def make_x_y_z_plot(df):\n",
    "    # colors = [\"#F7F3E3\",\"#7F7CAF\",\"#78C0E0\"]\n",
    "    option_colors = sns.color_palette()\n",
    "    colors =[option_colors[0],option_colors[1],option_colors[2]]\n",
    "    # create larger subplots\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(12, 8))\n",
    "        # Plotting\n",
    "    for i, (ax, variables) in enumerate(zip(axs.flat, [('lw_x', 'lw_y', 'lw_z'), ('lh_x', 'lh_y', 'lh_z'), ('la_x', 'la_y', 'la_z'), ('ra_x', 'ra_y', 'ra_z')])):\n",
    "        for variable, color in zip(variables, colors):\n",
    "            sns.lineplot(data=df, x='time_s', y=variable, ax=ax, linewidth=0.5, color=color, label=variable)\n",
    "        \n",
    "        ax.legend(loc='upper right', fontsize=12)\n",
    "        ax.set_title(' vs time_s '.join(variables), fontsize=16)\n",
    "        ax.set_xlabel('Time (s)', fontsize=14)\n",
    "        ax.set_ylabel('Value', fontsize=14)\n",
    "        ax.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# separate each activity based on the numebr\n",
    "def separate_activity(df,activity):\n",
    "    if activity == 99:\n",
    "        df_non_study = df[df['activity'] == 99]\n",
    "        return df_non_study\n",
    "    elif activity == 77:\n",
    "        df_clapping = df[df['activity'] == 77]\n",
    "        return df_clapping\n",
    "    elif activity ==4:\n",
    "        df_driving = df[df['activity'] == 4]\n",
    "        return df_driving\n",
    "    elif activity ==3:\n",
    "        df_ascending_stairs = df[df['activity'] == 3]\n",
    "        return df_ascending_stairs\n",
    "    elif activity ==2:\n",
    "        df_descending_stairs = df[df['activity'] == 2]\n",
    "        return df_descending_stairs\n",
    "    elif activity ==1:\n",
    "        df_walking = df[df['activity'] == 1]\n",
    "        return df_walking\n",
    "\n",
    "# show the period of each activity\n",
    "def show_x_y_z_plot(df,activity):\n",
    "    if activity == 99:\n",
    "        df_non_study = df[df['activity'] == 99]\n",
    "        # df_99 = df_99[(df_99['time_s'] >= 2600) & (df_99['time_s'] <= 2800)]\n",
    "        make_x_y_z_plot(df_99)\n",
    "    elif activity == 77:\n",
    "        df_77 = df[df['activity'] == 77]\n",
    "        # df_77 = df_77[(df_77['time_s'] >= 520) & (df_77['time_s'] <= 540)]\n",
    "        make_x_y_z_plot(df_77)\n",
    "    elif activity ==4:\n",
    "        df_4 = df[df['activity'] == 4]\n",
    "        # df_4 = df_4[(df_4['time_s'] >= 1400) & (df_4['time_s'] <= 1600)]\n",
    "        make_x_y_z_plot(df_4)\n",
    "    elif activity ==3:\n",
    "        df_3 = df[df['activity'] == 3]\n",
    "        # df_3 = df_3[(df_3['time_s'] >= 300) & (df_3['time_s'] <= 350)]\n",
    "        make_x_y_z_plot(df_3)\n",
    "    elif activity ==2:\n",
    "        df_2 = df[df['activity'] == 2]\n",
    "        # df_2 = df_2[(df_2['time_s'] >= 300) & (df_2['time_s'] <= 350)]\n",
    "        make_x_y_z_plot(df_2)\n",
    "    elif activity ==1:\n",
    "        df_1 = df[df['activity'] == 1]\n",
    "        # df_1 = df_1[(df_1['time_s'] >= 650) & (df_1['time_s'] <= 750)] \n",
    "        make_x_y_z_plot(df_1)\n",
    "\n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def standardize(df):\n",
    "    # initialize standard scaler\n",
    "    scaler = StandardScaler()\n",
    "    # select the columns to standardize\n",
    "    # columns_to_standardize = [\"lw_x\", \"lw_y\", \"lw_z\", \"lh_x\", \"lh_y\", \"lh_z\", \"la_x\", \"la_y\", \"la_z\", \"ra_x\", \"ra_y\", \"ra_z\"]\n",
    "    \n",
    "    columns_to_standardize = ['magnitude_0','magnitude_1', 'magnitude_2', 'magnitude_3']\n",
    "\n",
    "    # standardize the columns\n",
    "    df[columns_to_standardize] = scaler.fit_transform(df[columns_to_standardize])\n",
    "    # Convert the 'time_s' column to a datetime format if it isn't already\n",
    "    df['time_s'] = pd.to_datetime(df['time_s'], unit='s') \n",
    "    # Set the 'time_s' column as the index of the DataFrame\n",
    "    df.set_index('time_s', inplace=True)\n",
    "        # check if the data has been standardized\n",
    "    # print(df[columns_to_standardize].mean())  # should be close to 0\n",
    "    # print(df[columns_to_standardize].std())   # should be close to 1\n",
    "\n",
    "    # Reset the index back to RangeIndex\n",
    "    df.reset_index(inplace=True)\n",
    "    # Convert 'time_s' back to its original unit\n",
    "    df['time_s'] = df['time_s'].astype(int)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def butter_lowpass(cutoff, fs, order=3):\n",
    "    nyquist = 0.5 * fs\n",
    "    normal_cutoff = cutoff / nyquist\n",
    "    b, a = butter(order, normal_cutoff, btype='low', analog=False)\n",
    "    return b, a\n",
    "\n",
    "def butter_lowpass_filter(df, columns, cutoff, fs, order=3):\n",
    "    df_filtered = df.copy()\n",
    "    for column in columns:\n",
    "        b, a = butter_lowpass(cutoff, fs, order=order)\n",
    "        df_filtered[column] = filtfilt(b, a, df[column].values)\n",
    "    return df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_noise_filter(df):\n",
    "    # Assuming you want to apply a median filter and low-pass Butterworth filter\n",
    "    # columns_to_apply_filter = [\"lw_x\", \"lw_y\", \"lw_z\", \"lh_x\", \"lh_y\", \"lh_z\", \"la_x\", \"la_y\", \"la_z\", \"ra_x\", \"ra_y\", \"ra_z\"]\n",
    "    columns_to_apply_filter = ['magnitude_0','magnitude_1', 'magnitude_2', 'magnitude_3']\n",
    "\n",
    "    # Apply median filter to the specified columns\n",
    "    df_filtered = df.copy()\n",
    "    for column in columns_to_apply_filter:\n",
    "        df_filtered[column] = medfilt(df[column], kernel_size=5)  # Adjust kernel size as needed\n",
    "\n",
    "    # Apply low-pass Butterworth filter to the filtered data\n",
    "    cutoff_freq = 0.2  # Corner frequency in Hz\n",
    "    order = 3  # Butterworth filter order\n",
    "    fs = 100  # Sample rate (assuming equally spaced samples)\n",
    "    df_filtered = butter_lowpass_filter(df_filtered, columns_to_apply_filter, cutoff_freq, fs, order)\n",
    "\n",
    "    return df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize and segment\n",
    "def data_processed(df):\n",
    "    \n",
    "    df_new = standardize(df)\n",
    "    # Assuming your preprocessed data is stored in the DataFrame 'df_preprocessed'\n",
    "    df_filtered = apply_noise_filter(df_new)\n",
    "\n",
    "    # window_size = 3  # Size of each window in seconds\n",
    "    window_size = [5.12,10.24]\n",
    "    overlap = 0.5  # Overlap percentage (50%)\n",
    "    sampling_rate = 100  # Sampling rate of your data (samples per second)\n",
    "    #(i.e. 2.56s Ã— 100Hz *0.5 =  128 sample ref: paper1\n",
    "\n",
    "    # Calculate the number of data points in each window\n",
    "    window_length = int(window_size[0] * sampling_rate)\n",
    "\n",
    "    # Calculate the number of data points to shift the window by for the given overlap\n",
    "    shift_length = int(window_length * overlap)\n",
    "\n",
    "    # Initialize an empty list to store the segmented data\n",
    "    segmented_data = []\n",
    "\n",
    "    # Iterate over the data using a sliding window\n",
    "    start_index = 0\n",
    "    while start_index + window_length <= len(df_filtered):\n",
    "        end_index = start_index + window_length\n",
    "        segment = df_filtered.iloc[start_index:end_index]\n",
    "        segmented_data.append(segment)\n",
    "        start_index += shift_length\n",
    "\n",
    "    # Concatenate the segmented data into a new DataFrame\n",
    "    df_segmented = pd.concat(segmented_data)\n",
    "\n",
    "    # Reset the index of the segmented DataFrame\n",
    "    df_segmented.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "    return df_filtered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_vector_magnitude(df):\n",
    "    df_transformed = df.copy()\n",
    "    columns_to_calculate_VM = [\"lw_x\", \"lw_y\", \"lw_z\", \"lh_x\", \"lh_y\", \"lh_z\", \"la_x\", \"la_y\", \"la_z\", \"ra_x\", \"ra_y\", \"ra_z\"]\n",
    "    for i in range(0,len(columns_to_calculate_VM),3):\n",
    "        x_col = columns_to_calculate_VM[i]\n",
    "        y_col = columns_to_calculate_VM[i+1]\n",
    "        z_col = columns_to_calculate_VM[i+2]\n",
    "        magnitude_col = f\"magnitude_{i//3}\"\n",
    "        # Calculate vector magnitude while removing sensor orientation\n",
    "        df_transformed[magnitude_col] = np.sqrt(df[x_col]**2 + df[y_col]**2 + df[z_col]**2)\n",
    "    # print(df_transformed.columns)\n",
    "    return df_transformed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistical_extraction(data_processed,activity_name):\n",
    "    results = []\n",
    "    # calculate mean,std,variance,minimum,maximun,\n",
    "    mean = np.mean(data_processed)\n",
    "     # Calculate standard deviation\n",
    "    std = np.std(data_processed)\n",
    "\n",
    "    # Calculate variance\n",
    "    variance = np.var(data_processed)\n",
    "\n",
    "    # Calculate minimum\n",
    "    minimum = np.min(data_processed)\n",
    "\n",
    "    # Calculate maximum\n",
    "    maximum = np.max(data_processed)\n",
    "    result = {\n",
    "            'activity': f'{activity_name}',\n",
    "            'mean': mean,\n",
    "            'std': std,\n",
    "            'variance': variance,\n",
    "            'minimum': minimum,\n",
    "            'maximum': maximum\n",
    "        }\n",
    "        \n",
    "    results.append(result)\n",
    "\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.fftpack import fft, ifft\n",
    "\n",
    "variables = {\n",
    "    'magnitude_0': 'lw',\n",
    "    'magnitude_1': 'lh',\n",
    "    'magnitude_2': 'la',\n",
    "    'magnitude_3': 'ra'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FFT_plot(fft_results, labels):\n",
    "    N = len(fft_results[0])\n",
    "    sampling_rate = 100  # Assuming a sampling rate of 100 Hz\n",
    "    frequencies = np.fft.fftfreq(N, d=1/sampling_rate)\n",
    "    \n",
    "    # Plot the FFT results\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    sns.set_theme(context='notebook', style='darkgrid', palette=\"deep\")\n",
    "    \n",
    "    for fft_result, label in zip(fft_results, labels):\n",
    "        ax.plot(frequencies, fft_result, label=label)\n",
    "    \n",
    "    ax.set_xlabel('Frequency')\n",
    "    ax.set_ylabel('Magnitude')\n",
    "    ax.set_title('FFT Results')\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FFT_DWT(VM, variables):\n",
    "    fft_results = []\n",
    "    labels = []\n",
    "    for key, label in variables.items():\n",
    "        X = fft(VM[key].values)\n",
    "        fft_result = np.abs(X)\n",
    "        print(f'FFT result for {label}:', fft_result)\n",
    "        fft_results.append(fft_result)\n",
    "        labels.append(label)\n",
    "    \n",
    "    FFT_plot(fft_results, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_dict = {   \n",
    "        \"non-study activity\" : 99,\n",
    "        \"clapping\" : 77,\n",
    "        \"driving\" : 4,\n",
    "        \"ascending stairs\" : 3,\n",
    "        \"descending stairs\" : 2,\n",
    "        \"walking\": 1\n",
    "    }\n",
    "    # Folder path containing the CSV files\n",
    "folder_path = '../data/raw/physionet.org/files/accelerometry-walk-climb-drive/1.0.0/raw_accelerometry_data'\n",
    "file_path = '../data/raw/physionet.org/files/accelerometry-walk-climb-drive/1.0.0/raw_accelerometry_data/id00b70b13.csv'\n",
    "file_path2 = '../data/raw/physionet.org/files/accelerometry-walk-climb-drive/1.0.0/raw_accelerometry_data/id1c7e64ad.csv'\n",
    "plt.rcParams['figure.figsize'] = (20, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def method_can_do_all(file_path,activity):\n",
    "     # Generate the save path\n",
    "    results_before = '../output/transform/before noise filter/output'\n",
    "    save_path_before = f\"{results_before}_{activity}.jpg\"\n",
    "\n",
    "    results_after = '../output/transform/after noise filter/output'\n",
    "    save_path_after = f\"{results_after}_{activity}.jpg\"\n",
    "\n",
    "    os.makedirs(results_before,exist_ok=True)\n",
    "    os.makedirs(results_after,exist_ok=True)\n",
    "    \n",
    "    # 1.read and plot\n",
    "    df = pd.read_csv(file_path)\n",
    "    df_activity = separate_activity(df,activity)\n",
    "    df_activity_tranform = calculate_vector_magnitude(df_activity)\n",
    "    # print(df_activity_tranform)\n",
    "#     plot_all(df_activity_tranform,save_path_before)\n",
    "    \n",
    "\n",
    "    # 2.standardize and downsampling and segmentation\n",
    "    df_activity_processed = data_processed(df_activity_tranform)\n",
    "   \n",
    "    # 3. Noise filtering\n",
    "    filtered_data = apply_noise_filter(df_activity_processed)\n",
    "    \n",
    "    # 4.statical analysis\n",
    "    results = statistical_extraction(filtered_data,activity)\n",
    "\n",
    "    plot_all(filtered_data,save_path_after)\n",
    "    # FFT_DWT(filtered_data,variables)\n",
    "    # FFT_DWT(df_activity_tranform,variables)\n",
    "    \n",
    "   \n",
    "\n",
    "    return results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    results = []\n",
    "    # activity_name = 'walking'\n",
    "    for activity in activity_dict:\n",
    "        print(f\"Now activity is {activity} and the code is {activity_dict[activity]}\")    \n",
    "        activity_results = method_can_do_all(file_path2,activity_dict[activity])\n",
    "        # print(activity_results)\n",
    "\n",
    "        # results.extend(activity_results)\n",
    "    # Convert results to a DataFrame\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    # Save results to a CSV file\n",
    "    results_df.to_csv('statistical_resultsid.id1c7e64ad.csv', index=False)\n",
    "\n",
    "    # Display results as a table\n",
    "    # print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
