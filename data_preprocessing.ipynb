{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import matplotlib\n",
    "import calendar\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import plotly.io as pio\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import medfilt, butter, filtfilt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-08-16 13:29:23--  https://physionet.org/static/published-projects/accelerometry-walk-climb-drive/labeled-raw-accelerometry-data-captured-during-walking-stair-climbing-and-driving-1.0.0.zip\n",
      "Resolving physionet.org (physionet.org)... 18.18.42.54\n",
      "Connecting to physionet.org (physionet.org)|18.18.42.54|:443... connected.\n",
      "HTTP request sent, awaiting response... 416 Requested Range Not Satisfiable\n",
      "\n",
      "    The file is already fully retrieved; nothing to do.\n",
      "\n",
      "replace dataset/labeled-raw-accelerometry-data-captured-during-walking-stair-climbing-and-driving-1.0.0/LICENSE.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: ^C\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "!wget -c --timeout 10 https://physionet.org/static/published-projects/accelerometry-walk-climb-drive/labeled-raw-accelerometry-data-captured-during-walking-stair-climbing-and-driving-1.0.0.zip -O dataset.zip\n",
    "!unzip -d dataset -q dataset.zip\n",
    "!echo \"Done!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id00b70b13.csv id3e3e50c7.csv id82b9735c.csv idb221f542.csv idf540d82b.csv\n",
      "id079c763c.csv id4ea159a8.csv id86237981.csv idbae5a811.csv idf5e3678b.csv\n",
      "id1165e00c.csv id5308a7d6.csv id8af5374b.csv idc735fc09.csv idfc5f05e4.csv\n",
      "id1c7e64ad.csv id5993bf4a.csv id8e66893c.csv idc91a49d0.csv idff99de96.csv\n",
      "id1f372081.csv id650857ca.csv id9603e9c3.csv idd80ac2b4.csv\n",
      "id34e056c8.csv id687ab496.csv ida61e8ddf.csv idecc9265e.csv\n",
      "id37a54bbf.csv id7c20ee7a.csv idabd0c53c.csv idf1ce9a0f.csv\n"
     ]
    }
   ],
   "source": [
    "!ls dataset/labeled-raw-accelerometry-data-captured-during-walking-stair-climbing-and-driving-1.0.0/raw_accelerometry_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_single_file(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    return df   \n",
    "    # select by activity and plot them\n",
    "\n",
    "def read_all_data(folder_path):\n",
    "    # List to store the DataFrames\n",
    "    data_frames = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        \n",
    "        if filename.endswith(\".csv\"):\n",
    "            file_path = os.path.join(folder_path,filename)\n",
    "            df = pd.read_csv(file_path)\n",
    "            data_frames.append(df)\n",
    "        else:\n",
    "            print(f\"{filename} is not a csv file\")\n",
    "        \n",
    "    return data_frames \n",
    "    # print(df.head(5))\n",
    "    \n",
    "def plot_all(df,path):\n",
    "    sns.set_theme(context='notebook', style='darkgrid', palette=\"deep\")\n",
    "    # variables = [\"lw_x\", \"lw_y\", \"lw_z\", \"lh_x\", \"lh_y\", \"lh_z\", \"la_x\", \"la_y\", \"la_z\", \"ra_x\", \"ra_y\", \"ra_z\"]\n",
    "\n",
    "    variables = {\n",
    "    'magnitude_0': 'lw',\n",
    "    'magnitude_1': 'lh',\n",
    "    'magnitude_2': 'la',\n",
    "    'magnitude_3': 'ra'\n",
    "}\n",
    "   \n",
    "    # df.plot(x='time_s', y=variables)\n",
    "    df.plot(x='time_s', y=list(variables.keys()))\n",
    "    # tick_positions = range(0, 3034, 100) \n",
    "    # plt.xticks(tick_positions)\n",
    "    # Adding labels and a legend\n",
    "    \n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Values')\n",
    "    plt.legend([variables[var] for var in variables.keys()])\n",
    "    \n",
    "    # Displaying the plot\n",
    "    # plt.show()\n",
    "    plt.savefig(path)\n",
    "\n",
    "# plot(combined_df)\n",
    "\n",
    "\n",
    "\n",
    "def make_x_y_z_plot(df):\n",
    "    # colors = [\"#F7F3E3\",\"#7F7CAF\",\"#78C0E0\"]\n",
    "    option_colors = sns.color_palette()\n",
    "    colors =[option_colors[0],option_colors[1],option_colors[2]]\n",
    "    # create larger subplots\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(12, 8))\n",
    "        # Plotting\n",
    "    for i, (ax, variables) in enumerate(zip(axs.flat, [('lw_x', 'lw_y', 'lw_z'), ('lh_x', 'lh_y', 'lh_z'), ('la_x', 'la_y', 'la_z'), ('ra_x', 'ra_y', 'ra_z')])):\n",
    "        for variable, color in zip(variables, colors):\n",
    "            sns.lineplot(data=df, x='time_s', y=variable, ax=ax, linewidth=0.5, color=color, label=variable)\n",
    "        \n",
    "        ax.legend(loc='upper right', fontsize=12)\n",
    "        ax.set_title(' vs time_s '.join(variables), fontsize=16)\n",
    "        ax.set_xlabel('Time (s)', fontsize=14)\n",
    "        ax.set_ylabel('Value', fontsize=14)\n",
    "        ax.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# separate each activity based on the numebr\n",
    "def separate_activity(df,activity):\n",
    "    if activity == 99:\n",
    "        df_non_study = df[df['activity'] == 99]\n",
    "        return df_non_study\n",
    "    elif activity == 77:\n",
    "        df_clapping = df[df['activity'] == 77]\n",
    "        return df_clapping\n",
    "    elif activity ==4:\n",
    "        df_driving = df[df['activity'] == 4]\n",
    "        return df_driving\n",
    "    elif activity ==3:\n",
    "        df_ascending_stairs = df[df['activity'] == 3]\n",
    "        return df_ascending_stairs\n",
    "    elif activity ==2:\n",
    "        df_descending_stairs = df[df['activity'] == 2]\n",
    "        return df_descending_stairs\n",
    "    elif activity ==1:\n",
    "        df_walking = df[df['activity'] == 1]\n",
    "        return df_walking\n",
    "\n",
    "# show the period of each activity\n",
    "def show_x_y_z_plot(df,activity):\n",
    "    if activity == 99:\n",
    "        df_non_study = df[df['activity'] == 99]\n",
    "        # df_99 = df_99[(df_99['time_s'] >= 2600) & (df_99['time_s'] <= 2800)]\n",
    "        make_x_y_z_plot(df_99)\n",
    "    elif activity == 77:\n",
    "        df_77 = df[df['activity'] == 77]\n",
    "        # df_77 = df_77[(df_77['time_s'] >= 520) & (df_77['time_s'] <= 540)]\n",
    "        make_x_y_z_plot(df_77)\n",
    "    elif activity ==4:\n",
    "        df_4 = df[df['activity'] == 4]\n",
    "        # df_4 = df_4[(df_4['time_s'] >= 1400) & (df_4['time_s'] <= 1600)]\n",
    "        make_x_y_z_plot(df_4)\n",
    "    elif activity ==3:\n",
    "        df_3 = df[df['activity'] == 3]\n",
    "        # df_3 = df_3[(df_3['time_s'] >= 300) & (df_3['time_s'] <= 350)]\n",
    "        make_x_y_z_plot(df_3)\n",
    "    elif activity ==2:\n",
    "        df_2 = df[df['activity'] == 2]\n",
    "        # df_2 = df_2[(df_2['time_s'] >= 300) & (df_2['time_s'] <= 350)]\n",
    "        make_x_y_z_plot(df_2)\n",
    "    elif activity ==1:\n",
    "        df_1 = df[df['activity'] == 1]\n",
    "        # df_1 = df_1[(df_1['time_s'] >= 650) & (df_1['time_s'] <= 750)] \n",
    "        make_x_y_z_plot(df_1)\n",
    "\n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def standardize(df):\n",
    "    # initialize standard scaler\n",
    "    scaler = StandardScaler()\n",
    "    # select the columns to standardize\n",
    "    # columns_to_standardize = [\"lw_x\", \"lw_y\", \"lw_z\", \"lh_x\", \"lh_y\", \"lh_z\", \"la_x\", \"la_y\", \"la_z\", \"ra_x\", \"ra_y\", \"ra_z\"]\n",
    "    \n",
    "    columns_to_standardize = ['magnitude_0','magnitude_1', 'magnitude_2', 'magnitude_3']\n",
    "\n",
    "    # standardize the columns\n",
    "    df[columns_to_standardize] = scaler.fit_transform(df[columns_to_standardize])\n",
    "    # Convert the 'time_s' column to a datetime format if it isn't already\n",
    "    df['time_s'] = pd.to_datetime(df['time_s'], unit='s') \n",
    "    # Set the 'time_s' column as the index of the DataFrame\n",
    "    df.set_index('time_s', inplace=True)\n",
    "        # check if the data has been standardized\n",
    "    # print(df[columns_to_standardize].mean())  # should be close to 0\n",
    "    # print(df[columns_to_standardize].std())   # should be close to 1\n",
    "\n",
    "    # Reset the index back to RangeIndex\n",
    "    df.reset_index(inplace=True)\n",
    "    # Convert 'time_s' back to its original unit\n",
    "    df['time_s'] = df['time_s'].astype(int)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def butter_lowpass(cutoff, fs, order=3):\n",
    "    nyquist = 0.5 * fs\n",
    "    normal_cutoff = cutoff / nyquist\n",
    "    b, a = butter(order, normal_cutoff, btype='low', analog=False)\n",
    "    return b, a\n",
    "\n",
    "def butter_lowpass_filter(df, columns, cutoff, fs, order=3):\n",
    "    df_filtered = df.copy()\n",
    "    for column in columns:\n",
    "        b, a = butter_lowpass(cutoff, fs, order=order)\n",
    "        df_filtered[column] = filtfilt(b, a, df[column].values)\n",
    "    return df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_noise_filter(df):\n",
    "    # Assuming you want to apply a median filter and low-pass Butterworth filter\n",
    "    # columns_to_apply_filter = [\"lw_x\", \"lw_y\", \"lw_z\", \"lh_x\", \"lh_y\", \"lh_z\", \"la_x\", \"la_y\", \"la_z\", \"ra_x\", \"ra_y\", \"ra_z\"]\n",
    "    columns_to_apply_filter = ['magnitude_0','magnitude_1', 'magnitude_2', 'magnitude_3']\n",
    "\n",
    "    # Apply median filter to the specified columns\n",
    "    df_filtered = df.copy()\n",
    "    for column in columns_to_apply_filter:\n",
    "        df_filtered[column] = medfilt(df[column], kernel_size=5)  # Adjust kernel size as needed\n",
    "\n",
    "    # Apply low-pass Butterworth filter to the filtered data\n",
    "    cutoff_freq = 0.2  # Corner frequency in Hz\n",
    "    order = 3  # Butterworth filter order\n",
    "    fs = 100  # Sample rate (assuming equally spaced samples)\n",
    "    df_filtered = butter_lowpass_filter(df_filtered, columns_to_apply_filter, cutoff_freq, fs, order)\n",
    "\n",
    "    return df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize and segment\n",
    "def data_processed(df):\n",
    "    \n",
    "    df_new = standardize(df)\n",
    "    # Assuming your preprocessed data is stored in the DataFrame 'df_preprocessed'\n",
    "    df_filtered = apply_noise_filter(df_new)\n",
    "\n",
    "    # window_size = 3  # Size of each window in seconds\n",
    "    window_size = [5.12,10.24]\n",
    "    overlap = 0.5  # Overlap percentage (50%)\n",
    "    sampling_rate = 100  # Sampling rate of your data (samples per second)\n",
    "    #(i.e. 2.56s × 100Hz *0.5 =  128 sample ref: paper1\n",
    "\n",
    "    # Calculate the number of data points in each window\n",
    "    window_length = int(window_size[0] * sampling_rate)\n",
    "\n",
    "    # Calculate the number of data points to shift the window by for the given overlap\n",
    "    shift_length = int(window_length * overlap)\n",
    "\n",
    "    # Initialize an empty list to store the segmented data\n",
    "    segmented_data = []\n",
    "\n",
    "    # Iterate over the data using a sliding window\n",
    "    start_index = 0\n",
    "    while start_index + window_length <= len(df_filtered):\n",
    "        end_index = start_index + window_length\n",
    "        segment = df_filtered.iloc[start_index:end_index]\n",
    "        segmented_data.append(segment)\n",
    "        start_index += shift_length\n",
    "\n",
    "    # Concatenate the segmented data into a new DataFrame\n",
    "    df_segmented = pd.concat(segmented_data)\n",
    "\n",
    "    # Reset the index of the segmented DataFrame\n",
    "    df_segmented.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "    return df_filtered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_vector_magnitude(df):\n",
    "    df_transformed = df.copy()\n",
    "    columns_to_calculate_VM = [\"lw_x\", \"lw_y\", \"lw_z\", \"lh_x\", \"lh_y\", \"lh_z\", \"la_x\", \"la_y\", \"la_z\", \"ra_x\", \"ra_y\", \"ra_z\"]\n",
    "    for i in range(0,len(columns_to_calculate_VM),3):\n",
    "        x_col = columns_to_calculate_VM[i]\n",
    "        y_col = columns_to_calculate_VM[i+1]\n",
    "        z_col = columns_to_calculate_VM[i+2]\n",
    "        magnitude_col = f\"magnitude_{i//3}\"\n",
    "        # Calculate vector magnitude while removing sensor orientation\n",
    "        df_transformed[magnitude_col] = np.sqrt(df[x_col]**2 + df[y_col]**2 + df[z_col]**2)\n",
    "    # print(df_transformed.columns)\n",
    "    return df_transformed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistical_extraction(data_processed,activity_name):\n",
    "    results = []\n",
    "    # calculate mean,std,variance,minimum,maximun,\n",
    "    mean = np.mean(data_processed)\n",
    "     # Calculate standard deviation\n",
    "    std = np.std(data_processed)\n",
    "\n",
    "    # Calculate variance\n",
    "    variance = np.var(data_processed)\n",
    "\n",
    "    # Calculate minimum\n",
    "    minimum = np.min(data_processed)\n",
    "\n",
    "    # Calculate maximum\n",
    "    maximum = np.max(data_processed)\n",
    "    result = {\n",
    "            'activity': f'{activity_name}',\n",
    "            'mean': mean,\n",
    "            'std': std,\n",
    "            'variance': variance,\n",
    "            'minimum': minimum,\n",
    "            'maximum': maximum\n",
    "        }\n",
    "        \n",
    "    results.append(result)\n",
    "\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.fftpack import fft, ifft\n",
    "\n",
    "variables = {\n",
    "    'magnitude_0': 'lw',\n",
    "    'magnitude_1': 'lh',\n",
    "    'magnitude_2': 'la',\n",
    "    'magnitude_3': 'ra'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FFT_plot(fft_results, labels):\n",
    "    N = len(fft_results[0])\n",
    "    sampling_rate = 100  # Assuming a sampling rate of 100 Hz\n",
    "    frequencies = np.fft.fftfreq(N, d=1/sampling_rate)\n",
    "    \n",
    "    # Plot the FFT results\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    sns.set_theme(context='notebook', style='darkgrid', palette=\"deep\")\n",
    "    \n",
    "    for fft_result, label in zip(fft_results, labels):\n",
    "        ax.plot(frequencies, fft_result, label=label)\n",
    "    \n",
    "    ax.set_xlabel('Frequency')\n",
    "    ax.set_ylabel('Magnitude')\n",
    "    ax.set_title('FFT Results')\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FFT_DWT(VM, variables):\n",
    "    fft_results = []\n",
    "    labels = []\n",
    "    for key, label in variables.items():\n",
    "        X = fft(VM[key].values)\n",
    "        fft_result = np.abs(X)\n",
    "        print(f'FFT result for {label}:', fft_result)\n",
    "        fft_results.append(fft_result)\n",
    "        labels.append(label)\n",
    "    \n",
    "    FFT_plot(fft_results, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_dict = {   \n",
    "        \"non-study activity\" : 99,\n",
    "        \"clapping\" : 77,\n",
    "        \"driving\" : 4,\n",
    "        \"ascending stairs\" : 3,\n",
    "        \"descending stairs\" : 2,\n",
    "        \"walking\": 1\n",
    "    }\n",
    "    # Folder path containing the CSV files\n",
    "folder_path = 'dataset/labeled-raw-accelerometry-data-captured-during-walking-stair-climbing-and-driving-1.0.0/raw_accelerometry_data'\n",
    "file_path = 'dataset/labeled-raw-accelerometry-data-captured-during-walking-stair-climbing-and-driving-1.0.0/raw_accelerometry_data/raw_accelerometry_data/id00b70b13.csv'\n",
    "file_path2 = 'dataset/labeled-raw-accelerometry-data-captured-during-walking-stair-climbing-and-driving-1.0.0/raw_accelerometry_data/id1c7e64ad.csv'\n",
    "plt.rcParams['figure.figsize'] = (20, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def method_can_do_all(file_path,activity):\n",
    "     # Generate the save path\n",
    "    results_before = '../output/transform/before noise filter/output'\n",
    "    save_path_before = f\"{results_before}_{activity}.jpg\"\n",
    "\n",
    "    results_after = '../output/transform/after noise filter/output'\n",
    "    save_path_after = f\"{results_after}_{activity}.jpg\"\n",
    "\n",
    "    os.makedirs(results_before,exist_ok=True)\n",
    "    os.makedirs(results_after,exist_ok=True)\n",
    "    \n",
    "    # 1.read and plot\n",
    "    df = pd.read_csv(file_path)\n",
    "    df_activity = separate_activity(df,activity)\n",
    "    df_activity_tranform = calculate_vector_magnitude(df_activity)\n",
    "    # print(df_activity_tranform)\n",
    "    plot_all(df_activity_tranform,save_path_before)\n",
    "    \n",
    "\n",
    "    # 2.standardize and downsampling and segmentation\n",
    "    df_activity_processed = data_processed(df_activity_tranform)\n",
    "   \n",
    "    # 3. Noise filtering\n",
    "    filtered_data = apply_noise_filter(df_activity_processed)\n",
    "    \n",
    "    # 4.statical analysis\n",
    "    results = statistical_extraction(filtered_data,activity)\n",
    "\n",
    "    plot_all(filtered_data,save_path_after)\n",
    "    # FFT_DWT(filtered_data,variables)\n",
    "    # FFT_DWT(df_activity_tranform,variables)\n",
    "    \n",
    "   \n",
    "\n",
    "    return results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    results = []\n",
    "    # activity_name = 'walking'\n",
    "    for activity in activity_dict:\n",
    "        print(f\"Now activity is {activity} and the code is {activity_dict[activity]}\")    \n",
    "        activity_results = method_can_do_all(file_path2,activity_dict[activity])\n",
    "        # print(activity_results)\n",
    "\n",
    "        # results.extend(activity_results)\n",
    "    # Convert results to a DataFrame\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    # Save results to a CSV file\n",
    "    results_df.to_csv('statistical_resultsid.id1c7e64ad.csv', index=False)\n",
    "\n",
    "    # Display results as a table\n",
    "    # print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now activity is non-study activity and the code is 99\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plot_all' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m----> 2\u001b[0m     main()\n",
      "Cell \u001b[0;32mIn[36], line 7\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m activity \u001b[39min\u001b[39;00m activity_dict:\n\u001b[1;32m      6\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNow activity is \u001b[39m\u001b[39m{\u001b[39;00mactivity\u001b[39m}\u001b[39;00m\u001b[39m and the code is \u001b[39m\u001b[39m{\u001b[39;00mactivity_dict[activity]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)    \n\u001b[0;32m----> 7\u001b[0m     activity_results \u001b[39m=\u001b[39m method_can_do_all(file_path2,activity_dict[activity])\n\u001b[1;32m      8\u001b[0m     \u001b[39m# print(activity_results)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \n\u001b[1;32m     10\u001b[0m     \u001b[39m# results.extend(activity_results)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[39m# Convert results to a DataFrame\u001b[39;00m\n\u001b[1;32m     13\u001b[0m results_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(results)\n",
      "Cell \u001b[0;32mIn[35], line 29\u001b[0m, in \u001b[0;36mmethod_can_do_all\u001b[0;34m(file_path, activity)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[39m# 4.statical analysis\u001b[39;00m\n\u001b[1;32m     27\u001b[0m results \u001b[39m=\u001b[39m statistical_extraction(filtered_data,activity)\n\u001b[0;32m---> 29\u001b[0m plot_all(filtered_data,save_path_after)\n\u001b[1;32m     30\u001b[0m \u001b[39m# FFT_DWT(filtered_data,variables)\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[39m# FFT_DWT(df_activity_tranform,variables)\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[39mreturn\u001b[39;00m results\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plot_all' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HDA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
